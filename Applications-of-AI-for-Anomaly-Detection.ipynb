{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries that will be needed for the lab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import os, datetime\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "%load_ext tensorboard\n",
    "\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "data_path = './data'"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pct_anomalies = .01\n",
    "!python preprocess_data.py --pct_anomalies $pct_anomalies"
   ],
   "id": "75743372d45f3119"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "filename = './preprocessed_data_full.pkl'\n",
    "input_file = open(filename,'rb')\n",
    "preprocessed_data = pickle.load(input_file)\n",
    "input_file.close()"
   ],
   "id": "f31d53a51567ee2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for key in preprocessed_data:\n",
    "    print(key)\n",
    "le = preprocessed_data['le']\n",
    "x_train = preprocessed_data['x_train']\n",
    "y_train = preprocessed_data['y_train']\n",
    "x_test = preprocessed_data['x_test']\n",
    "y_test = preprocessed_data['y_test']"
   ],
   "id": "923c931dc026859f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Normalize the testing and training data using the MinMaxScaler from the scikit learn package\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Make sure to only fit the scaler on the training data\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# convert the data to FP32\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n"
   ],
   "id": "a15287000da3a393"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "input_dim = x_train.shape[1]\n",
    "\n",
    "# model hyperparameters\n",
    "batch_size = 512\n",
    "\n",
    "latent_dim = 4\n",
    "\n",
    "max_epochs = 10 "
   ],
   "id": "9af3a2a0d8f1483a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# The encoder will consist of a number of dense layers that decrease in size \n",
    "# as we taper down towards the bottleneck of the network, the latent space\n",
    "input_data = Input(shape=(input_dim,), name='encoder_input')\n",
    "\n",
    "# hidden layers\n",
    "encoder = Dense(96,activation='tanh', name='encoder_1')(input_data)\n",
    "encoder = Dropout(.1)(encoder)\n",
    "encoder = Dense(64,activation='tanh', name='encoder_2')(encoder)\n",
    "encoder = Dropout(.1)(encoder)\n",
    "encoder = Dense(48,activation='tanh', name='encoder_3')(encoder)\n",
    "encoder = Dropout(.1)(encoder)\n",
    "encoder = Dense(16,activation='tanh', name='encoder_4')(encoder)\n",
    "encoder = Dropout(.1)(encoder)\n",
    "\n",
    "# bottleneck layer\n",
    "latent_encoding = Dense(latent_dim, activation='linear', name='latent_encoding')(encoder)"
   ],
   "id": "3e7c99864eb8ba31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "encoder_model = Model(input_data, latent_encoding)\n",
    "\n",
    "encoder_model.summary()"
   ],
   "id": "ccd7bf735b099637"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plot_model(\n",
    "    encoder_model, \n",
    "    to_file='./data/encoder_model.png', \n",
    "    show_shapes=True, \n",
    "    show_layer_names=True, \n",
    "    rankdir='TB' # TB for top to bottom, LR for left to right\n",
    ")\n",
    "\n",
    "Image(filename='./data/encoder_model.png')"
   ],
   "id": "81790e5a20580b60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# The decoder network is a mirror image of the encoder network\n",
    "decoder = Dense(16, activation='tanh', name='decoder_1')(latent_encoding)\n",
    "decoder = Dropout(.1)(decoder)\n",
    "decoder = Dense(48, activation='tanh', name='decoder_2')(decoder)\n",
    "decoder = Dropout(.1)(decoder)\n",
    "decoder = Dense(64, activation='tanh', name='decoder_3')(decoder)\n",
    "decoder = Dropout(.1)(decoder)\n",
    "decoder = Dense(96, activation='tanh', name='decoder_4')(decoder)\n",
    "decoder = Dropout(.1)(decoder)\n",
    "\n",
    "# The output is the same dimension as the input data we are reconstructing\n",
    "reconstructed_data = Dense(input_dim, activation='linear', name='reconstructed_data')(decoder)"
   ],
   "id": "ad16637933dc8e75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "autoencoder_model = Model(input_data, reconstructed_data)\n",
    "\n",
    "autoencoder_model.summary()"
   ],
   "id": "b2c8200a8f811aed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plot_model(\n",
    "    autoencoder_model, \n",
    "    to_file='autoencoder_model.png', \n",
    "    show_shapes=True, \n",
    "    show_layer_names=True, \n",
    "    rankdir='TB' # TB for top to bottom, LR for left to right\n",
    ")\n",
    "\n",
    "Image(filename='autoencoder_model.png')"
   ],
   "id": "9bbdf3b0219ed213"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "opt = optimizers.Adam(learning_rate=.00001)\n",
    "\n",
    "autoencoder_model.compile(optimizer=opt, loss='mse', metrics=['accuracy'])"
   ],
   "id": "5a82f226570273c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir,profile_batch=0,update_freq='epoch',histogram_freq=1)\n",
    "\n",
    "train_history = autoencoder_model.fit(x_train, x_train,\n",
    "        shuffle=True,\n",
    "        epochs=max_epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test),\n",
    "        callbacks=[tensorboard_callback])"
   ],
   "id": "afb2f35db8150203"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.plot(train_history.history['loss'])\n",
    "plt.plot(train_history.history['val_loss'])\n",
    "plt.legend(['loss on train data', 'loss on validation data'])"
   ],
   "id": "9b21e1dde3c662c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%tensorboard --logdir logs",
   "id": "a06fc346cb3e720c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Reconstruct the data using our trainined autoencoder model.\n",
    "x_test_recon = autoencoder_model.predict(x_test)\n",
    "\n",
    "# the reconstruction score is the mean of the reconstruction errors (relatively high scores are anomalous)\n",
    "reconstruction_scores = np.mean((x_test - x_test_recon)**2, axis=1)"
   ],
   "id": "c7881d452ab4d7f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# store the reconstruction data in a Pandas dataframe\n",
    "anomaly_data = pd.DataFrame({'recon_score':reconstruction_scores})\n",
    "\n",
    "# if our reconstruction scores our normally distributed we can use their statistics\n",
    "anomaly_data.describe()"
   ],
   "id": "9de87ad4f2f90957"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# plotting the density will give us an idea of how the reconstruction scores are distributed\n",
    "plt.xlabel('Reconstruction Score')\n",
    "anomaly_data['recon_score'].plot.hist(bins=200, range=[-.01, .03])"
   ],
   "id": "8c620fffab085ab3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def convert_label_to_binary(label_encoder, labels):\n",
    "    normal_idx = np.where(label_encoder.classes_ == 'normal.')[0][0]\n",
    "    my_labels = labels.copy()\n",
    "    my_labels[my_labels != normal_idx] = 1 \n",
    "    my_labels[my_labels == normal_idx] = 0\n",
    "    return my_labels"
   ],
   "id": "daa3dc4473a0c416"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# convert our labels to binary\n",
    "binary_labels = convert_label_to_binary(le, y_test)\n",
    "\n",
    "# add the binary labels to our anomaly dataframe\n",
    "anomaly_data['binary_labels'] = binary_labels\n",
    "\n",
    "# let's check if the reconstruction statistics are different for labeled anomalies\n",
    "anomaly_data.groupby(by='binary_labels').describe()"
   ],
   "id": "a78f5d5efd429aab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fpr, tpr, thresholds = roc_curve(binary_labels, reconstruction_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(fpr, tpr, lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='lime', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ],
   "id": "bf87396ac93f9522"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We can pick the threshold based on maximizing the true positive rate (tpr) \n",
    "# and minimizing the false positive rate (fpr)\n",
    "optimal_threshold_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_threshold_idx]\n",
    "print(optimal_threshold)"
   ],
   "id": "81c8438b60023f31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Or we assume our reconstructions are normally distributed and label anomalies as those\n",
    "# that are a number of standard deviations away from the mean\n",
    "recon_mean = np.mean(reconstruction_scores)\n",
    "recon_stddev = np.std(reconstruction_scores)\n",
    "\n",
    "stats_threshold = recon_mean + 5*recon_stddev\n",
    "print(stats_threshold)"
   ],
   "id": "63090db1507b8a0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# play around here and find the threshold that works for you\n",
    "\n",
    "#thresh = optimal_threshold\n",
    "thresh = stats_threshold\n",
    "\n",
    "\n",
    "\n",
    "print(thresh)\n",
    "\n",
    "pred_labels = (reconstruction_scores > thresh).astype(int)\n",
    "\n",
    "results = confusion_matrix(binary_labels, pred_labels) "
   ],
   "id": "4aa606a030ea2a78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print ('Confusion Matrix: ')\n",
    "\n",
    "def plot_confusion_matrix(cm, target_names, title='Confusion Matrix', cmap=plt.cm.Greens):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    width, height = cm.shape\n",
    "\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            plt.annotate(str(cm[x][y]), xy=(y, x), \n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "\n",
    "\n",
    "plot_confusion_matrix(results, ['Normal','Anomaly'])"
   ],
   "id": "ed099aad1a3346cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# subset the test data so that we are only looking at the predicted anomalies\n",
    "x_test_df = pd.DataFrame(x_test)\n",
    "\n",
    "test_anomalies = x_test_df[pred_labels.astype('bool')]\n",
    "\n",
    "y_test_df = pd.DataFrame(y_test)\n",
    "test_anomalies_labels = y_test_df[pred_labels.astype('bool')]\n",
    "\n",
    "# encode the test anomalies into latent space\n",
    "encoded_test_anomalies = encoder_model.predict(test_anomalies)"
   ],
   "id": "c8fc316c16838f85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# apply KMeans to the data in order to create clusters of anomalies\n",
    "kmeans = KMeans(n_clusters=10, random_state=123)\n",
    "kmeans.fit(encoded_test_anomalies)"
   ],
   "id": "22ee434df96383f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "clusters = pd.DataFrame({'cluster':kmeans.labels_, 'label':test_anomalies_labels[0]})\n",
    "\n",
    "most_frequent_labels = clusters.groupby('cluster').label.value_counts()\n",
    "\n",
    "print(most_frequent_labels)"
   ],
   "id": "631cf124594aade5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# this dictionary will map our cluster values to the labels that appear most frequently\n",
    "cluster_to_label = {}\n",
    "for cluster in range(0, 10):\n",
    "    label = most_frequent_labels[cluster].index[0]\n",
    "    cluster_to_label[cluster] = label\n",
    "\n",
    "# we then replace the clusters inplace by their label values\n",
    "clusters.cluster.replace(cluster_to_label, inplace=True)"
   ],
   "id": "8c9c4825318656a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "target_names = np.unique(list(clusters.label))\n",
    "cm = confusion_matrix(clusters.label, clusters.cluster)\n",
    "\n",
    "print ('Confusion Matrix :')\n",
    "# Calculate accuracy\n",
    "total_correct_predictions = np.sum(np.diag(cm))  # Sum of true positives\n",
    "total_predictions = np.sum(cm)  # Total number of predictions\n",
    "accuracy = total_correct_predictions / total_predictions if total_predictions > 0 else 0.0\n",
    "\n",
    "print(f'Final Accuracy: {accuracy:.2f}')\n",
    "\n",
    "def plot_confusion_matrix(cm,target_names, title='Confusion matrix', cmap=plt.cm.Greens):\n",
    "    plt.figure(figsize=(10,10),)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    width, height = cm.shape\n",
    "\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            plt.annotate(str(cm[x][y]), xy=(y, x), \n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "plot_confusion_matrix(cm,target_names)\n"
   ],
   "id": "12a40c0b5c12c683"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Threat Data Analysis Insights\n",
    "\n",
    "This report provides insights into detected threats, focusing on:\n",
    "\n",
    "### 1. Distribution of Threat Types\n",
    "\n",
    "- **Insight**: This shows the frequency of each threat type (e.g., Malware, Phishing, DDoS) in the dataset.\n",
    "  \n",
    "### 2. IP Address Patterns\n",
    "\n",
    "- **Insight**: This identifies which IP addresses are most frequently associated with threats.\n",
    "  \n",
    "### 3. Threat Count Summary\n",
    "\n",
    "- **Insight**: This includes statistics like the minimum, maximum, average, and total count of threats observed.\n",
    "\n",
    "### 4. Temporal Distribution of Threats\n",
    "\n",
    "- **Insight**: This shows how threats are distributed over the time series in the dataset.\n"
   ],
   "id": "e583875afac49c53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "56999c8ceb934027"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import sweetviz as sv\n",
    "\n",
    "# Sample Threat Data - simulate timestamps, IP addresses, and threat types\n",
    "time_series = pd.date_range(start='2023-10-01', periods=100, freq='H')\n",
    "threat_types = ['Malware', 'Phishing', 'DDoS', 'Brute Force', 'SQL Injection']\n",
    "ip_addresses = [f'192.168.1.{i}' for i in range(1, 101)]\n",
    "\n",
    "threats_data = pd.DataFrame({\n",
    "    'Time': time_series,\n",
    "    'IP Address': [random.choice(ip_addresses) for _ in range(100)],\n",
    "    'Threat Type': [random.choice(threat_types) for _ in range(100)],\n",
    "    'Threat Count': [random.randint(1, 15) for _ in range(100)]\n",
    "})\n",
    "\n",
    "# Generate Sweetviz report for Threat Data only\n",
    "report = sv.analyze(threats_data)\n",
    "report.show_html(\"cybersecurity_dashboard.html\")\n"
   ],
   "id": "cb8842add3b10f47",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
